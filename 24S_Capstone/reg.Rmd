---
title: "EmoRegression"
author: "Xiayuanshan Gao"
date: "2024-07-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(lubridate)
library(extrafont) 
library(ggcorrplot)
library(corrr) 

library(reshape2)
library(knitr)
library(kableExtra)

#install.packages("nnet")
library(nnet)
```

# New NDVI 

```{r}
rev2<- st_read('output_files/PPR_with_NDVI_Amenity.geojson') 
```



# Review data with PPR amenity info

```{r cars}
rev_origin <- read.csv('output_files/PrepforRegression.csv')
```

```{r}
rev_origin <- rev_origin %>%
  distinct(ParkName, .keep_all = TRUE)
```



```{r}
#rev[is.na(rev)] <- 0
```

# Census data 

```{r pressure, echo=FALSE}
variables <- c("B01001_001E",
  "B01001_002E",
  "B01001_026E",
   "B01001_007E",
  "B01001_012E",
   "B01001_016E",
 "B01001_020E","B02001_002E","B02001_003E", "B03001_003E", "B02001_004E", "B19001_002E","B19001_003E", "B19001_006E", "B19001_008E",
"B19001_010E", "B19001_012E","B07001_001E","B07001_017E","B07001_033E",
"B07001_049E",
"B07001_065E",
  "B15001_002E",
 "B15001_003E",
"B15001_005E",
"B15001_009E",
"B15001_010E",
   "B25001_001E",
  "B06012_002E",
  "B25002_003E",
"B25006_002E",
  "B25006_001E",
  "B19013_001E",
"B25077_001E",
"B25031_001",
"B08133_001",
"B19013_001E"
)

acs_2023 <- get_acs(
  geography = "block group",
  variables = variables,
  year = 2022,
  state = "PA",
  county = "Philadelphia",
  geometry = TRUE,
  survey = "acs5",
  output = "wide"
)
```

```{r}
acs_2023 <- acs_2023 %>%
  rename(
    totalPop = B01001_001E,
    male = B01001_002E,
    female = B01001_026E,
    age_18_24 = B01001_007E,
    age_25_44 = B01001_012E,
    age_45_64 = B01001_016E,
    age_65plus = B01001_020E,
    white = B02001_002E,
    popblack = B02001_003E,
    latino = B03001_003E,
    other_race = B02001_004E,
    income_less_20k = B19001_002E,
    income_20k_40k = B19001_003E,
    income_40k_60k = B19001_006E,
    income_60k_80k = B19001_008E,
    income_80k_100k = B19001_010E,
    income_more_100k = B19001_012E,
    migration_total = B07001_001E,
    migration_within_county = B07001_017E,
    migration_different_county_same_state = B07001_033E,
    migration_different_state = B07001_049E,
    migration_from_abroad = B07001_065E,
    edu_less_high_school = B15001_002E,
    edu_high_school_grad = B15001_003E,
    edu_some_college = B15001_005E,
    edu_bachelors = B15001_009E,
    edu_grad_professional = B15001_010E,
    total_housing_units = B25001_001E,
    total_poverty = B06012_002E,
    vacant_units = B25002_003E,
    white_houseOwner = B25006_002E,
    total_occupied_units = B25006_001E,
    median_HH_income = B19013_001E,
    houseprice = B25077_001E,
    rent = B25031_001E,
    travetimetowork = B08133_001E
  ) %>%
  #select(-starts_with("B"),-NAME) %>%
  mutate(
    WhiteHOrate = white_houseOwner / total_occupied_units,
    pctBachelors = ifelse(totalPop > 0, (edu_bachelors + edu_grad_professional) / totalPop, 0),
    pctPoverty = ifelse(totalPop > 0, total_poverty / totalPop, 0),
    pctWhitePop = ifelse(totalPop > 0, white / totalPop, 0),
    pctLatinoPop = ifelse(totalPop > 0, latino / totalPop, 0),
    pctBlackPop = ifelse(totalPop > 0, popblack / totalPop, 0),
    pctMigration = ifelse(totalPop > 0, migration_total / totalPop, 0),
    pctForeignMig = ifelse(totalPop > 0, migration_from_abroad / totalPop, 0),
    pct2540 = ifelse(totalPop > 0,  age_25_44 / totalPop, 0),
    landArea = st_area(geometry), 
    popDensity = totalPop / as.numeric(landArea)
  )
```

```{r}
rev <- rev %>%
  rowwise() %>%
  mutate(geometry = st_sfc(st_point(c(Long, Lat)), crs = 4326)) %>%
  st_sf() 
acs_2023 <- st_as_sf(acs_2023, crs = 4326)
```

```{r}
acs_2023 <- acs_2023 %>%
  mutate(vacantPct = vacant_units/total_housing_units,
         seniorPct = age_65plus / totalPop,
         kidPct = 1- (age_18_24+age_25_44+age_45_64)/totalPop)
```


```{r}
acs_selected <- acs_2023[c("totalPop","median_HH_income", "vacantPct", "seniorPct", "kidPct","pctWhitePop","pctBlackPop","popDensity",
                           "WhiteHOrate","houseprice")]

```
```{r}
rev <- st_transform(rev, st_crs(acs_selected))
```

```{r}
#rev <- st_join(rev, acs_selected, left = TRUE)
```

```{r}
acs_centroids <- st_centroid(acs_selected)

# Find the nearest features
nearest_indices <- st_nearest_feature(rev, acs_centroids)
rev<- st_join(rev, acs_selected[nearest_indices, ], join = st_nearest_feature)
```


```{r}
write.csv(st_drop_geometry(rev), "output_files/ForRregression_rev_with_acs.csv", row.names = FALSE)
```


# Neighborhood - Fixed Effect

```{r}
nhood <- st_read('nhood.json')

nhood <- st_transform(nhood, st_crs(rev))
```

```{r}
nhood <- nhood[c("NAME","geometry")]
```

```{r}
rev <- st_join(rev, nhood[c("NAME")], left = TRUE)
```

# Planning District

```{r}
dis <- st_read('Planning_Districts.geojson')

dis <- st_transform(dis, st_crs(rev))
```

```{r}
rev <- st_join(rev, dis[c("DIST_NAME")], left = TRUE)
```

# Spatial Vars

## rail 
```{r}
railline <- st_read('PaRailLines2024_03.geojson') %>% st_transform(st_crs(rev))

railline <- st_make_valid(railline)

# distance to the closest 
railline_nearest_fts <- st_nearest_feature(rev,railline,left=TRUE)

rev$railline_distance <- as.double(st_distance(rev,railline[railline_nearest_fts,], by_element=TRUE))
```

## school
```{r}
school <- st_read('https://opendata.arcgis.com/datasets/d46a7e59e2c246c891fbee778759717e_0.geojson') %>% st_transform(st_crs(rev))

# distance to the closest school
school <- st_make_valid(school)

# distance to the closest public art
school_nearest_fts <- st_nearest_feature(rev,school,left=TRUE)

rev$school_distance <- as.double(st_distance(rev,school[school_nearest_fts,], by_element=TRUE))
```
## pub art

```{r public art location}
pubArt <- st_read('https://phl.carto.com/api/v2/sql?filename=percent_for_art_public&format=geojson&skipfields=cartodb_id&q=SELECT+*+FROM+percent_for_art_public')%>%
  st_transform(st_crs(rev))

pubArt <- st_make_valid(pubArt)

# distance to the closest public art
art_nearest_fts <- st_nearest_feature(rev,pubArt,left=TRUE)

rev$art_distance <- as.double(st_distance(rev,pubArt[art_nearest_fts,], by_element=TRUE))
```
## highway

```{r highway}
highway <- st_read('https://opendata.arcgis.com/datasets/0a0d06a0c78e47c5bb73cde26630db07_0.geojson') %>%
  st_transform(st_crs(rev))

# shortest distance to the closest highway

highway <- st_make_valid(highway)

# distance to the closest public art
highway_nearest_fts <- st_nearest_feature(rev,highway,left=TRUE)

rev$highway_distance <- as.double(st_distance(rev,highway[highway_nearest_fts,], by_element=TRUE))
```
## demolished housing 
```{r demolished housing}

demoHousing <- st_read('https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+demolitions&filename=demolitions&format=geojson&skipfields=cartodb_id') %>% st_transform(st_crs(rev))

# shortest distance to the closest bikenet
demoHousing <- st_make_valid(demoHousing)

# distance to the closest public art
demoHousing_nearest_fts <- st_nearest_feature(rev,demoHousing,left=TRUE)

rev$demoHousing_distance <- as.double(st_distance(rev,demoHousing[demoHousing_nearest_fts,], by_element=TRUE))
```


```{r}
st_write(rev, "output_files/ForRregression_wo_counts.geojson", driver = "GeoJSON")
```
```{r}
rev <- st_read('output_files/ForRregression_wo_counts.geojson') 
```

## crime

```{r}
crime23 <- read.csv('incidents_part1_part2.csv')

crime23 <- crime23[!is.na(crime23$point_x) & !is.na(crime23$point_y), ]

crime23_sf <- st_as_sf(crime23, coords = c("point_x", "point_y"), crs = 4326)
```

```{r}
crime23_sf  <- st_transform(crime23_sf , st_crs(rev))

# Create a buffer around each park point (half mile ~ 804.7 meters)
rev_buffered <- st_buffer(rev, dist = 804.7)  

# Perform spatial join to count trees within each buffer
park_crime23  <- st_join(rev_buffered, crime23_sf , join = st_intersects,left = TRUE) %>%
  group_by(ParkName) %>%
  summarize(crime23_count = n(), .groups = 'drop')

rev <- st_drop_geometry(rev)
park_crime23 <- st_drop_geometry(park_crime23)

rev <- rev %>%
  left_join(park_crime23, by = "ParkName")
```

```{r}
rev <- st_as_sf(rev, coords = c("Long", "Lat"), crs = st_crs(acs_2023))
```


## trees 

```{r}
#Get Data on trees in Philadelphia
trees <- st_read('https://opendata.arcgis.com/api/v3/datasets/5abe042f2927486891c049cf064338cb_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1')%>%
  st_transform('EPSG:4326')

```
```{r}
trees <- st_transform(trees, st_crs(rev))


# Perform spatial join to count trees within each buffer
park_trees <- st_join(rev_buffered, trees, join = st_intersects,left = TRUE) %>%
  group_by(ParkName) %>%
  summarize(tree_count = n(), .groups = 'drop')

rev <- st_drop_geometry(rev)
park_trees <- st_drop_geometry(park_trees)

rev <- rev %>%
  left_join(park_trees, by = "ParkName")

```

```{r}
st_write(rev, "reg_final.geojson", driver = "GeoJSON")
```
# Filling in the missing value

```{r}
#replace_na_with_mean <- function(x) {
  #replace(x, is.na(x), mean(x, na.rm = TRUE))
#}

# Apply the function to each column in the data frame
#rev <- rev %>%
  #mutate(across(everything(), ~ replace_na_with_mean(.)))
```
```{r}
# Function to replace 0 and NA with mean
#replace_zero_and_na_with_mean <- function(x) {
  #mean_value <- mean(x[x != 0], na.rm = TRUE)
  #replace(x, x == 0 | is.na(x), mean_value)
#}

# Apply the function to specific columns
#rev <- rev %>%
  #mutate(across(c(Shape__Area, median_NDVI), ~ replace_zero_and_na_with_mean(.)))

```

```{r}
st_write(rev, "reg_final.geojson", driver = "GeoJSON", append = FALSE)
```
```{r}
rev <- as.data.frame(rev)

columns_to_remove <- c("index_right", "PUBLIC_NAME", "PARENT_NAME", "OBJECTID", "buffered_geometry")

rev <- rev[, !(names(rev) %in% columns_to_remove)]

```

```{r}
rev <- as.data.frame(rev)

columns_to_remove <- c("highway_distance")

rev <- rev[, !(names(rev) %in% columns_to_remove)]

```

```{r}
write.csv(rev, "output_files/reg_final.csv", append = FALSE)
```
```{r}
rev <- st_read("output_files/reg_final.csv")
```

```{r}
rev <- rev %>% 
  filter(ParkName != "Fish Hatchery at Pleasant Hill Park")
```

```{r}
rev <- rev %>%
  left_join(rev_origin %>% 
              dplyr::select(ParkName, Discovery_percentage_weighted, Engagement_percentage_weighted,
                            Frustration...Annoyance_percentage_weighted, Inspiration_percentage_weighted,
                            Relaxation_percentage_weighted, Sorrow_percentage_weighted), 
            by = "ParkName") %>%
  # Convert character columns to numeric before using coalesce
  mutate(
    Discovery_percentage_weighted.x = as.numeric(Discovery_percentage_weighted.x),
    Discovery_percentage_weighted.y = as.numeric(Discovery_percentage_weighted.y),
    Engagement_percentage_weighted.x = as.numeric(Engagement_percentage_weighted.x),
    Engagement_percentage_weighted.y = as.numeric(Engagement_percentage_weighted.y),
    Frustration...Annoyance_percentage_weighted.x = as.numeric(Frustration...Annoyance_percentage_weighted.x),
    Frustration...Annoyance_percentage_weighted.y = as.numeric(Frustration...Annoyance_percentage_weighted.y),
    Inspiration_percentage_weighted.x = as.numeric(Inspiration_percentage_weighted.x),
    Inspiration_percentage_weighted.y = as.numeric(Inspiration_percentage_weighted.y),
    Relaxation_percentage_weighted.x = as.numeric(Relaxation_percentage_weighted.x),
    Relaxation_percentage_weighted.y = as.numeric(Relaxation_percentage_weighted.y),
    Sorrow_percentage_weighted.x = as.numeric(Sorrow_percentage_weighted.x),
    Sorrow_percentage_weighted.y = as.numeric(Sorrow_percentage_weighted.y)
  ) %>%
  # Now coalesce the columns to combine
  mutate(
    Discovery_percentage_weighted = coalesce(Discovery_percentage_weighted.y, Discovery_percentage_weighted.x),
    Engagement_percentage_weighted = coalesce(Engagement_percentage_weighted.y, Engagement_percentage_weighted.x),
    Frustration...Annoyance_percentage_weighted = coalesce(Frustration...Annoyance_percentage_weighted.y, Frustration...Annoyance_percentage_weighted.x),
    Inspiration_percentage_weighted = coalesce(Inspiration_percentage_weighted.y, Inspiration_percentage_weighted.x),
    Relaxation_percentage_weighted = coalesce(Relaxation_percentage_weighted.y, Relaxation_percentage_weighted.x),
    Sorrow_percentage_weighted = coalesce(Sorrow_percentage_weighted.y, Sorrow_percentage_weighted.x)
  ) %>%
  # Remove the duplicate columns after updating
  dplyr::select(-Discovery_percentage_weighted.x, -Discovery_percentage_weighted.y, 
                -Engagement_percentage_weighted.x, -Engagement_percentage_weighted.y,
                -Frustration...Annoyance_percentage_weighted.x, -Frustration...Annoyance_percentage_weighted.y,
                -Inspiration_percentage_weighted.x, -Inspiration_percentage_weighted.y,
                -Relaxation_percentage_weighted.x, -Relaxation_percentage_weighted.y,
                -Sorrow_percentage_weighted.x, -Sorrow_percentage_weighted.y)

```

```{r}
columns_to_remove <- c("field_1")
rev <- rev[, !(names(rev) %in% columns_to_remove)]
```

```{r}
columns_to_remove <- c("Playstreets_Locations")
rev <- rev[, !(names(rev) %in% columns_to_remove)]
```

```{r}
write.csv(rev, "output_files/reg_final.csv", append = FALSE)
```

# Correlation & Regression!

## Corr

```{r}
# Function to replace 0 and NA with mean
columns_to_exclude <- c("ParkName", "NAME", "DIST_NAME")

# Convert all other columns to numeric
rev <- rev %>%
  mutate(across(-all_of(columns_to_exclude), as.numeric))

# Remove the excluded columns for correlation analysis
rev_corr <- rev[, !(names(rev) %in% columns_to_exclude)]
```

```{r fig.height=24, fig.width=24}
rev %>%
  correlate() %>%
  autoplot() +
  geom_text(aes(label = round(r, digits = 2)), size = 5, order = "hclust", type = "upper", tl.cex = 3)+
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust =0),
        axis.text.y = element_text(size = 12),
        legend.text = element_text(size = 12),
          legend.key.size = unit(1.5, "cm")) +    # Adjust legend bar size
  guides(fill = guide_colorbar(barwidth = 2, barheight = 50)) 
```
```{r fig.height=24, fig.width=24}

library(reshape2)
# Compute the correlation matrix
cor_matrix <- cor(rev_corr, use = "complete.obs")
cor_matrix_df <- as.data.frame(cor_matrix)

# Add a column for the row names (variables)
cor_matrix_df$term <- rownames(cor_matrix_df)

# Specify the columns of interest
columns_of_interest <- c("Inspiration_percentage_weighted", "Relaxation_percentage_weighted", 
                         "Engagement_percentage_weighted", "Discovery_percentage_weighted", 
                         "Frustration...Annoyance_percentage_weighted", "Sorrow_percentage_weighted")

# Reshape the correlation matrix for plotting using melt
filtered_cor_matrix_long <- melt(cor_matrix_df, id.vars = "term")

# Filter the correlation matrix to include correlations with columns of interest on the x-axis
filtered_cor_matrix_long <- filtered_cor_matrix_long %>%
  filter(variable %in% columns_of_interest)

# Define the coolwarm color palette
coolwarm_palette <- c("#3B4CC0", "#778ECF", "#F6F7FB", "#DC3C00", "#B40426")

# Plot the filtered correlations
ggplot(filtered_cor_matrix_long, aes(x = variable, y = term, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), size = 4) +
  scale_fill_gradientn(colors = coolwarm_palette, limits = c(-1, 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1, size = 12),
        axis.text.y = element_text(size = 14),
        legend.text = element_text(size = 14),
        legend.title = element_text(size = 16),
        legend.key.size = unit(1.5, "cm")) +  # Adjust legend bar size
  guides(fill = guide_colorbar(barwidth = 2, barheight = 50))

```


```{r fig.height=12, fig.width=30}
# Compute the correlation matrix
cor_matrix <- correlate(rev_corr)
cor_matrix_df <- as.data.frame(cor_matrix)

# Specify the columns of interest
columns_of_interest <- c("Inspiration_percentage_weighted", "Relaxation_percentage_weighted", "Engagement_percentage_weighted", "Discovery_percentage_weighted", "Frustration...Annoyance_percentage_weighted", "Sorrow_percentage_weighted")

# Filter the correlation matrix to only include correlations involving the columns of interest
filtered_cor_matrix_df <- cor_matrix_df %>%
  filter(term %in% columns_of_interest)

# Reshape the filtered correlation matrix for plotting
filtered_cor_matrix_long <- melt(filtered_cor_matrix_df, id.vars = "term")
# Filter out rows where the y variable is also in the columns of interest
filtered_cor_matrix_long <- filtered_cor_matrix_long %>%
  filter(!variable %in% columns_of_interest)

# Define the coolwarm color palette
coolwarm_palette <- c("#B40426", "#DC3C00", "#F6F7FB", "#778ECF","#3B4CC0")

# Plot the filtered correlations
ggplot(filtered_cor_matrix_long, aes(y = term, x = variable, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), size = 6,order = "hclust", type = "upper") +
  scale_fill_gradientn(colors = coolwarm_palette, 
                       limits = c(-1, 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 20),
        axis.text.y = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
          legend.key.size = unit(1.5, "cm")) +    # Adjust legend bar size
  guides(fill = guide_colorbar(barwidth = 2, barheight = 50)) 
```

```{r}
emotions <- c("Inspiration_percentage_weighted", 
              "Relaxation_percentage_weighted", 
              "Engagement_percentage_weighted", 
              "Discovery_percentage_weighted", 
              "Frustration...Annoyance_percentage_weighted", 
              "Sorrow_percentage_weighted")
```


```{r}
 # Function to separate data by mean, calculate group means, and perform t-tests
calculate_group_means <- function(data, emotion_col, emotions) {
  mean_value <- mean(data[[emotion_col]], na.rm = TRUE)
  low_group <- data[data[[emotion_col]] <= mean_value, ]
  high_group <- data[data[[emotion_col]] > mean_value, ]
  
  # Remove non-numeric columns and other emotional columns
  columns_to_exclude <- emotions
  low_group <- low_group[ , !names(low_group) %in% columns_to_exclude]
  low_group <- low_group[sapply(low_group, is.numeric)]
  high_group <- high_group[ , !names(high_group) %in% columns_to_exclude]
  high_group <- high_group[sapply(high_group, is.numeric)]
  
  low_means <- colMeans(low_group, na.rm = TRUE)
  high_means <- colMeans(high_group, na.rm = TRUE)
  
  # Calculate p-values using t-test
  p_values <- sapply(names(low_means), function(var) {
    if (var %in% c("Low_N", "High_N")) {
      return(NA)
    } else {
      t_test <- t.test(low_group[[var]], high_group[[var]], na.rm = TRUE)
      return(t_test$p.value)
    }
  })
  
  result <- data.frame(Low_Mean = low_means,
                       High_Mean = high_means,
                       P_Value = p_values)
  
  # Include the counts in the header
  n_info <- list(Low_N = nrow(low_group), High_N = nrow(high_group))
  
  return(list(result = result, n_info = n_info))
}

# Apply the function to each emotional column
results_list <- lapply(emotions, calculate_group_means, data = rev, emotions = emotions)

# Create a pretty table for each emotion
for (i in 1:length(results_list)) {
  emotion_name <- emotions[i]
  result_data <- results_list[[i]]$result
  n_info <- results_list[[i]]$n_info
  
  # Adjust numbers to avoid scientific notation
  result_data$Low_Mean <- format(result_data$Low_Mean, scientific = FALSE, digits = 2)
  result_data$High_Mean <- format(result_data$High_Mean, scientific = FALSE, digits = 2)
  result_data$P_Value <- format(result_data$P_Value, scientific = FALSE, digits = 4)
  
  # Create a table
  print(paste("Results for:", emotion_name))
  table_output <- kable(result_data, format = "html", caption = paste("Mean Values by", emotion_name, "Group")) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                  full_width = FALSE,
                  position = "left") %>%
    add_header_above(c("Variables" = 1,
                       paste0("Low Group (n=", n_info$Low_N, ")"), 
                       paste0("High Group (n=", n_info$High_N, ")"),
                       "T-Test P-Value" = 1)) %>%
    row_spec(0, bold = TRUE) %>%
    column_spec(1, bold = TRUE)
  
  # Display the table
  print(table_output)
 
}
```
```{r}
install.packages("webshot")
webshot::install_phantomjs()

# List of emotions to iterate through
emotions <- c("Inspiration", "Relaxation", "Engagement", "Discovery", "Frustration & Annoyance", "Sorrow")

# Loop through each emotion to save the table
for (emotion_name in emotions) {
  # Print table for the current emotion
  print(paste("Results for:", emotion_name))
  
  # Generate table output
  table_output <- kable(result_data, format = "html", 
                        caption = paste("Mean Values by", emotion_name, "Group")) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                  full_width = FALSE,
                  position = "left") %>%
    add_header_above(c("Variables" = 1, 
                       paste0("Low Group (n=", n_info$Low_N, ")"), 
                       paste0("High Group (n=", n_info$High_N, ")"), 
                       "T-Test P-Value" = 1)) %>%
    row_spec(0, bold = TRUE) %>%
    column_spec(1, bold = TRUE)
  
  # Define the filename
  file_name_html <- paste0("table_", gsub(" ", "_", emotion_name), ".html")
  file_name_png <- paste0("table_", gsub(" ", "_", emotion_name), ".png")
  
  # Save table as HTML
  save_kable(table_output, file = file_name_html)
  
  # Save table as PNG (optional, requires webshot or similar package)
  # Install the webshot package and phantomjs if not already installed
  # webshot::install_phantomjs()
  webshot::webshot(file_name_html, file_name_png, vwidth = 800, vheight = 600)
  
  print(paste("Saved table for", emotion_name))
}

```

## Regs
```{r}
mod_insp <- lm(Inspiration_percentage_weighted ~  log(median_HH_income) + median_NDVI+ NAME, data =rev)
summary(mod_insp)
```
```{r}
mod_insp_multinom <- multinom(Inspiration_percentage_weighted ~ log(railline_distance) + log(crime23_count) + 
                         log(median_HH_income) + totalPop + median_NDVI + Adult_Exercise_Equipment , data = rev)
summary(mod_insp_multinom)
```

```{r}
mod_relax <- lm(Relaxation_percentage_weighted ~  median_NDVI+ Pools_Sprayground +NAME, data =rev)
summary(mod_relax)
```


















